### RNN using R 


softmax <- function(x){
                numerator = exp(x-max(x))
                denominator = matrix(colSums(exp(x-max(x))), nrow = dim(numerator)[1], dim(numerator)[2], byrow =T)
                numerator/denominator}
                
                

library(ramify)
ClipGradient <- function(gradients, maxValue) {
                                for(i in 1:length(gradients)){
                                                             gradients[[i]] = clip(gradients[[i]], 
                                                             .min = -maxValue, 
                                                             .max = maxValue)
                                                             }
                                              gradients
                                              }
                                              
                                              
 initialize_parameters <- function(n_a, n_x, n_y) {
                Wax = (matrix(rnorm(n_a*n_x,0,1), nrow = n_a, ncol = n_x))*0.01
                Waa = (matrix(rnorm(n_a*n_a,0,1), nrow = n_a, ncol = n_a))*0.01
                Wya = (matrix(rnorm(n_y*n_a,0,1), nrow = n_y, ncol = n_a))*0.01
                b = array(0, c(n_a,1))
                by = array(0, c(n_y,1))
                parameters = list("Wax" = Wax, "Waa"= Waa, "Wya"= Wya, "b"= b,"by"= by)
  
                parameters}


### Forward Propagation

rnn_step_forward <- function(parameters, a_prev,xt) {
        
        Waa = parameters[['Waa']]
        Wax = parameters[['Wax']]
        Wya = parameters[['Wya']]
        by = parameters[['by']]
        b = parameters[['b']]
        
        Wax.aa = Wax%*%xt + Waa %*%a_prev 
        a_next = tanh(Wax.aa + matrix(b, nrow = dim(Wax.aa)[1], dim(Wax.aa)[2]))
        Wya.a_next <- Wya %*% a_next
        yt_pred = softmax(Wya.a_next + matrix(by, nrow = dim(Wya.a_next)[1], dim(Wya.a_next)[2]))
        
        list("a_next" = a_next, "p_t" = yt_pred)
}



rnn_forward <- function(X, Y, a0, parameters, vocab_size = 27) {
        
        x = list()
        a = list()
        y_hat = list()
        a[[1]] =  a0    # in python this is a[-1] = np.copy(a0)
        loss = 0
        
        for (t in 1:length(X)){
                
                              x[[t]] = array(0, c(vocab_size,1))
                              if (is.na(X[t]) != TRUE){ x[[t]][X[t]] = 1}
                              rnn_step_forward.output = rnn_step_forward(parameters, a[[t]], x[[t]])
                              a[[t+1]] = rnn_step_forward.output$a_next
                              y_hat[[t]] = rnn_step_forward.output$p_t
                
                              loss = loss - log(y_hat[[t]][Y[t],1])
                              }
        
        cache = list("y_hat" = y_hat, "a" =a, "x" = x)
        
        list("loss" = loss, "cache" =  cache)
        
}



### Backward Propagation

rnn_step_backward <- function(dy, gradients, parameters, x, a, a_prev) {
        
        gradients[["dWya"]] = gradients[["dWya"]] + dy %*% t(a)
        gradients[["dby"]] = gradients[["dby"]] + dy
        da = t(parameters[["Wya"]]) %*% dy + gradients[["da_next"]]
        daraw = (1- a^2)*da
        gradients[["db"]] = gradients[["db"]]  + daraw
        gradients[["dWax"]] = gradients[["dWax"]] + daraw %*% t(x)
        gradients[["dWaa"]] = gradients[["dWaa"]] + daraw %*% t(a_prev)
        gradients[["da_next"]] = gradients[["da_next"]] +  t(parameters[["Waa"]]) %*% daraw
        
        gradients
}

rnn_backward <- function(X, Y, parameters, cache) {
        
        gradients = list()
        
        y_hat = cache$y_hat
        a = cache$a
        x = cache$x
        
        Waa = parameters[['Waa']]
        Wax = parameters[['Wax']]
        Wya = parameters[['Wya']]
        by = parameters[['by']]
        b = parameters[['b']]
        
        
        gradients[['dWax']] = array(0, c(dim(Wax)[1],dim(Wax)[2]))
        gradients[['dWaa']] = array(0, c(dim(Waa)[1],dim(Waa)[2]))
        gradients[['dWya']] = array(0, c(dim(Wya)[1],dim(Wya)[2])) 
        gradients[['db']] = array(0, c(dim(b)[1],dim(b)[2])) 
        gradients[['dby']] = array(0, c(dim(by)[1],dim(by)[2])) 
        gradients[['da_next']] = array(0, c(dim(a[[1]])[1],dim(a[[1]])[2])) # python uses a[0]
        
        
        for (t in (rev(1:length(X)))){
                                     dy = y_hat[[t]]
                                     dy[Y[t]] = dy[Y[t]] - 1
                                     gradients = rnn_step_backward(dy, gradients, parameters, x[[t]], a[[t+1]], a[[t]]) 
                                                                               # Note! in python this is a[t] and a[t-1]
                                     }
        
        list("gradients" = gradients, "a" = a)
}



### Update Weights

update_parameters <- function(parameters, gradients, lr) {
                                                        parameters[['Wax']] = parameters[['Wax']] -lr*gradients[['dWax']]
                                                        parameters[['Waa']] = parameters[['Waa']] -lr*gradients[['dWaa']]
                                                        parameters[['Wya']] = parameters[['Wya']] -lr*gradients[['dWya']]
                                                        parameters[['b']] = parameters[['b']] -lr*gradients[['db']]
                                                        parameters[['by']] = parameters[['by']] -lr*gradients[['dby']]
        
                                                        parameters
                                                        }



### Optimizer

optimize <- function(X, Y, a_prev, parameters, learning_rate = 0.005) {
        source("rnn_forward.R")
        source("rnn_step_forward.R")
        source("softmax.R")
        rnn_forward.output = rnn_forward(X, Y, a_prev, parameters)
        loss = rnn_forward.output$loss
        cache = rnn_forward.output$cache
        
        source("rnn_backward.R")
        source("rnn_step_backward.R")
        rnn_backward.output = rnn_backward(X, Y, parameters, cache)  
        gradients = rnn_backward.output$gradients
        a = rnn_backward.output$a
        
        source("ClipGradient.R")
        gradients = ClipGradient(gradients, 5)
        
        source("update_parameters.R")
        parameters = update_parameters(parameters, gradients, learning_rate)
        
        list("loss" = loss, "gradients" = gradients, "a_Final" = a[(length(X)+1)], "parameters" = parameters )
        #Note! in python this is a[len(X)-1] as a[] starts from -1 to len(X) or len(a) = len(X)+ 1
        
}
